{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6ad95ff09749bebea8b36958b1db1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021bd759d17649d6a2778f3473d878ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chakr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chakr\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c5f1a354ba4a71a776e6b6b11cc202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a6f8afe5a446478aacfe98e7c550c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc33e2d0b6ca406582dc0115d4344fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ce87573a804744b57f0be2c0a0fa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc542272a54abdac7e19b5f79402d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b48a21c3a1049fa8e58c0b78d40a73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<pad> my name is aryaborty</s>'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def sum_pegasus(text):\n",
    "  \n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\")\n",
    "  text=text\n",
    "  tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  summary = model.generate(**tokens)\n",
    "  return tokenizer.decode(summary[0])\n",
    "\n",
    "\n",
    "s = sum_pegasus('my name is arya chakraborty')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def translate(text,target,max_length):\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')\n",
    "  tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n",
    "\n",
    "  translator=pipeline('translation', model=model, tokenizer=tokenizer,src_lang='eng_Latn', tgt_lang=language[target],max_length=max_length)\n",
    "  translated_text=translator(text)\n",
    "  return translated_text[0]['translation_text']\n",
    "\n",
    "\n",
    "language = {'Acehnese (Arabic script)':\t'ace_Arab',\n",
    "            'Acehnese (Latin script)':\t'ace_Latn',\n",
    "'Mesopotamian Arabic':\t'acm_Arab',\n",
    "'Taizzi-Adeni Arabic':\t'acq_Arab',\n",
    "'Tunisian Arabic':\t'aeb_Arab',\n",
    "'Afrikaans':\t'afr_Latn',\n",
    "'South Levantine Arabic':\t'ajp_Arab',\n",
    "'Akan':\t'aka_Latn',\n",
    "'Amharic'\t:'amh_Ethi',\n",
    "'North Levantine Arabic':\t'apc_Arab',\n",
    "'Modern Standard Arabic':\t'arb_Arab',\n",
    "'Modern Standard Arabic (Romanized)':\t'arb_Latn',\n",
    "'Najdi Arabic':\t'ars_Arab',\n",
    "'Moroccan Arabic':\t'ary_Arab',\n",
    "'Egyptian Arabic'\t:'arz_Arab',\n",
    "'Assamese':\t'asm_Beng',\n",
    "'Asturian' :\t'ast_Latn',\n",
    "'Awadhi':\t'awa_Deva',\n",
    "'Central Aymara':\t'ayr_Latn',\n",
    "'South Azerbaijani':\t'azb_Arab',\n",
    "'North Azerbaijani':\t'azj_Latn',\n",
    "'Bashkir':\t'bak_Cyrl',\n",
    "'Bambara':\t'bam_Latn',\n",
    "'Balinese':\t'ban_Latn',\n",
    "'Belarusian':\t'bel_Cyrl',\n",
    "'Bemba':\t'bem_Latn',\n",
    "'Bengali':\t'ben_Beng',\n",
    "'Bhojpuri':\t'bho_Deva',\n",
    "'Banjar (Arabic script)':\t'bjn_Arab',\n",
    "'Banjar (Latin script)':\t'bjn_Latn',\n",
    "'Standard Tibetan':\t'bod_Tibt',\n",
    "'Bosnian':\t'bos_Latn',\n",
    "'Buginese':\t'bug_Latn',\n",
    "'Bulgarian':\t'bul_Cyrl',\n",
    "'Catalan':\t'cat_Latn',\n",
    "'Cebuano':\t'ceb_Latn',\n",
    "'Czech':\t'ces_Latn',\n",
    "'Chokwe':\t'cjk_Latn',\n",
    "'Central Kurdish':\t'ckb_Arab',\n",
    "'Crimean Tatar':\t'crh_Latn',\n",
    "'Welsh':\t'cym_Latn',\n",
    "'Danish':\t'dan_Latn',\n",
    "'German':\t'deu_Latn',\n",
    "'Southwestern Dinka':\t'dik_Latn',\n",
    "'Dyula':\t'dyu_Latn',\n",
    "'Dzongkha'\t:'dzo_Tibt',\n",
    "'Greek':\t'ell_Grek',\n",
    "'English':\t'eng_Latn',\n",
    "'Esperanto':\t'epo_Latn',\n",
    "'Estonian':\t'est_Latn',\n",
    "'Basque':\t'eus_Latn',\n",
    "'Ewe':\t'ewe_Latn',\n",
    "'Faroese':\t'fao_Latn',\n",
    "'Fijian':\t'fij_Latn',\n",
    "'Finnish':\t'fin_Latn',\n",
    "'Fon':\t'fon_Latn',\n",
    "'French':\t'fra_Latn',\n",
    "'Friulian':\t'fur_Latn',\n",
    "'Nigerian Fulfulde':\t'fuv_Latn',\n",
    "'Scottish Gaelic':\t'gla_Latn',\n",
    "'Irish':\t'gle_Latn',\n",
    "'Galician':\t'glg_Latn',\n",
    "'Guarani':\t'grn_Latn',\n",
    "'Gujarati':\t'guj_Gujr',\n",
    "'Haitian Creole':\t'hat_Latn',\n",
    "'Hausa':\t'hau_Latn',\n",
    "'Hebrew':\t'heb_Hebr',\n",
    "'Hindi':\t'hin_Deva',\n",
    "'Chhattisgarhi':\t'hne_Deva',\n",
    "'Croatian':\t'hrv_Latn',\n",
    "'Hungarian':\t'hun_Latn',\n",
    "'Armenian':\t'hye_Armn',\n",
    "'Igbo':\t'ibo_Latn',\n",
    "'Ilocano':\t'ilo_Latn',\n",
    "'Indonesian':\t'ind_Latn',\n",
    "'Icelandic':\t'isl_Latn',\n",
    "'Italian':\t'ita_Latn',\n",
    "'Javanese':\t'jav_Latn',\n",
    "'Japanese':\t'jpn_Jpan',\n",
    "'Kabyle':\t'kab_Latn',\n",
    "'Jingpho':\t'kac_Latn',\n",
    "'Kamba':\t'kam_Latn',\n",
    "'Kannada':\t'kan_Knda',\n",
    "'Kashmiri (Arabic script)':\t'kas_Arab',\n",
    "'Kashmiri (Devanagari script)':\t'kas_Deva',\n",
    "'Georgian':\t'kat_Geor',\n",
    "'Central Kanuri (Arabic script)':\t'knc_Arab',\n",
    "'Central Kanuri (Latin script)':\t'knc_Latn',\n",
    "'Kazakh':\t'kaz_Cyrl',\n",
    "'Kabiyè\t': 'kbp_Latn',\n",
    "'Kabuverdianu':\t'kea_Latn',\n",
    "'Khmer':\t'khm_Khmr',\n",
    "'Kikuyu':\t'kik_Latn',\n",
    "'Kinyarwanda':\t'kin_Latn',\n",
    "'Kyrgyz':\t'kir_Cyrl',\n",
    "'Kimbundu':\t'kmb_Latn',\n",
    "'Northern Kurdish':\t'kmr_Latn',\n",
    "'Kikongo':\t'kon_Latn',\n",
    "'Korean':\t'kor_Hang',\n",
    "'Lao':\t'lao_Laoo',\n",
    "'Ligurian':\t'lij_Latn',\n",
    "'Limburgish':\t'lim_Latn',\n",
    "'Lingala':\t'lin_Latn',\n",
    "'Lithuanian':\t'lit_Latn',\n",
    "'Lombard':\t'lmo_Latn',\n",
    "'Latgalian':\t'ltg_Latn',\n",
    "'Luxembourgish':\t'ltz_Latn',\n",
    "'Luba-Kasai':\t'lua_Latn',\n",
    "'Ganda':\t'lug_Latn',\n",
    "'Luo':\t'luo_Latn',\n",
    "'Mizo':\t'lus_Latn',\n",
    "'Standard Latvian':\t'lvs_Latn',\n",
    "'Magahi':\t'mag_Deva',\n",
    "'Maithili':\t'mai_Deva',\n",
    "'Malayalam':\t'mal_Mlym',\n",
    "'Marathi':\t'mar_Deva',\n",
    "'Minangkabau (Arabic script)':\t'min_Arab',\n",
    "'Minangkabau (Latin script)':\t'min_Latn',\n",
    "'Macedonian':\t'mkd_Cyrl',\n",
    "'Plateau Malagasy':\t'plt_Latn',\n",
    "'Maltese':\t'mlt_Latn',\n",
    "'Meitei (Bengali script)':\t'mni_Beng',\n",
    "'Halh Mongolian\t':'khk_Cyrl',\n",
    "'Mossi':\t'mos_Latn',\n",
    "'Maori':\t'mri_Latn',\n",
    "'Burmese':\t'mya_Mymr',\n",
    "'Dutch':\t'nld_Latn',\n",
    "'Norwegian Nynorsk':\t'nno_Latn',\n",
    "'Norwegian Bokmål':\t'nob_Latn',\n",
    "'Nepali':\t'npi_Deva',\n",
    "'Northern Sotho':\t'nso_Latn',\n",
    "'Nuer':\t'nus_Latn',\n",
    "'Nyanja':\t'nya_Latn',\n",
    "'Occitan':\t'oci_Latn',\n",
    "'West Central Oromo':\t'gaz_Latn',\n",
    "'Odia':\t'ory_Orya',\n",
    "'Pangasinan':\t'pag_Latn',\n",
    "'Eastern Panjabi':\t'pan_Guru',\n",
    "'Papiamento':\t'pap_Latn',\n",
    "'Western Persian':\t'pes_Arab',\n",
    "'Polish\t':'pol_Latn',\n",
    "'Portuguese':\t'por_Latn',\n",
    "'Dari':\t'prs_Arab',\n",
    "'Southern Pashto':\t'pbt_Arab',\n",
    "'Ayacucho Quechua':\t'quy_Latn',\n",
    "'Romanian':\t'ron_Latn',\n",
    "'Rundi':\t'run_Latn',\n",
    "'Russian':\t'rus_Cyrl',\n",
    "'Sango':\t'sag_Latn',\n",
    "'Sanskrit':\t'san_Deva',\n",
    "'Santali':\t'sat_Olck',\n",
    "\"Sicilian\":\t'scn_Latn',\n",
    "'Shan':\t'shn_Mymr',\n",
    "'Sinhala':\t'sin_Sinh',\n",
    "'Slovak'\t:'slk_Latn',\n",
    "'Slovenian':\t'slv_Latn',\n",
    "'Samoan'\t:'smo_Latn',\n",
    "'Shona':\t'sna_Latn',\n",
    "'Sindhi'\t:'snd_Arab',\n",
    "'Somali'\t:'som_Latn',\n",
    "'Southern Sotho'\t:'sot_Latn',\n",
    "'Spanish'\t: 'spa_Latn',\n",
    "'Tosk Albanian'\t:'als_Latn',\n",
    "'Sardinian'\t:'srd_Latn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आप अब अपने ब्राउज़र में अपने Streamlit एप्लिकेशन देख सकते हैं'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('You can now view your Streamlit app in your browser', 'Hindi',max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40dfc05937e43eca07daad51c6c9d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chakr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chakr\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44837d89a5e8492d9b46c3a71730b4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba2b78ac13945849ae6f2976d956b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bb5a569b624188b242804dfccc5ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a321b5b20029471c950dfd309de8eadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d750d6f229f942b4be5f21cb8ca5fc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  text2=text\n",
    "  tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  summary = model.generate(**tokens)\n",
    "  return tokenizer.decode(summary[0])\n",
    "\n",
    "\n",
    "s = sum_pegasus('my name is arya chakraborty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"cherryberry01/Re_Sum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = PeftModel.from_pretrained(model, \"cherryberry01/Re_Sum\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Born and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team. He made his international debut in 2008 and quickly became a key player in the ODI team. He made his T20I debut in 2010 and later made scoring iTest debut in 2011. In 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time. During 2014 T20 World Cup, he set a record for the most runs scored in the tournament. In 2018, he achieved yet another milestone, becoming the world's top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game. His form continued in 2019, when he became the first player to score 20,000 international runs in a single decade. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(text,return_tensors='pt')\n",
    "summary=tokenizer.decode(\n",
    "    model.generate(input_ids=inputs['input_ids'], max_new_tokens=200, temperature=1.2001, do_sample=True)[0],\n",
    "    skip_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kohli is the only Indian cricketer to hold the number one spot in all three formats of the game. He made his international debut in 2008 and quickly became a key player in the ODI team. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
